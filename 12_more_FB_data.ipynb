{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a9c9472",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================\n",
    "# 12_more_FB_data.ipynb\n",
    "# Purpose: collect and preprocess more data from Facebook Marketing API to test divergent validity.\n",
    "# Author: Yuanmo He\n",
    "#===============================================================================\n",
    "\n",
    "from pysocialwatcher import watcherAPI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "\n",
    "# We use the library pysocialwatcher to query Facebook Marketing watcherAPI\n",
    "# details about the library setup https://github.com/joaopalotti/pySocialWatcher\n",
    "# more guidance https://worldbank.github.io/connectivity_mapping/intro.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659326cb",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b998377",
   "metadata": {},
   "source": [
    "## get the brands' interest ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69073086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FB_credentials.csv is where Facebook credentials are saved.\n",
    "watcher = watcherAPI(api_version=\"15.0\")\n",
    "watcher.load_credentials_file(\"FB_credentials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949190ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the Facebook interest ids for the brands\n",
    "brands_on_twitter = pd.read_csv(\"../brands_on_twitter.csv\")\n",
    "brands = brands_on_twitter['brand']\n",
    "rowlst = []\n",
    "errorlst = []\n",
    "\n",
    "for i in range(len(brands)):\n",
    "    \n",
    "    try:\n",
    "        rowlst.append(dict(watcher.get_interests_given_query(brands[i]).loc[0]))\n",
    "        \n",
    "    except Exception as e:\n",
    "        errorlst.append({i: e})\n",
    "    print(i)\n",
    "    \n",
    "print(len(rowlst)) # 315 brands founded as of November 2022\n",
    "print(len(errorlst)) # 26 errors, all KeyError, so need manual check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dab83b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(rowlst)) # 315 brands founded as of November 2022\n",
    "print(len(errorlst)) # 26 errors, all KeyError, so need manual check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109a8dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save interest id and manual check \n",
    "FB_interest_id = pd.DataFrame(rowlst)\n",
    "error_ids = [list(i.keys())[0] for i in errorlst]\n",
    "FB_interest_id.insert(0, 'brand', brands.drop(error_ids).reset_index(drop = True))\n",
    "FB_interest_id = FB_interest_id.rename(columns = {'name': \"FB_name\"})\n",
    "FB_interest_id.to_excel('FB_interest_id_raw.xlsx', index = False) \n",
    "\n",
    "# saving to excel because I need to manually add the other brands, and alter werid ones\n",
    "# label the weird ones in FB_interest_id_raw.xlsx, check and alter in FB_interest_id.xlsx\n",
    "# also add the ones that cannot be found automatically\n",
    "# Available and correct ones are saved as FB_interest_id.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f5f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually check special cases\n",
    "t = watcher.get_interests_given_query(\"Zenni\")\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fca7ebe",
   "metadata": {},
   "source": [
    "No longer available:\n",
    "\n",
    "Toys \"R\" Us, Jared, Hanes, Dockers, USA Today, The New York Times, The Onion, Fox News, CNN, CBS News, 60 Minutes, Central Market, the Hill, 48 Hours, American Housewife, Art Van Furniture, BabiesRUs, Century 21, Daily Caller, \n",
    "Flywheel Sports, Free Speech TV, Fusion News, Gordmans, MSNBC, NBC News, NCAA Women's Basketball, Natinal Football Leauge, Newsmax, Pier 1 Imports, The Wall Street Journal, The Washington Post, Warby Parker, Zenni Optical, world net daily\n",
    "\n",
    "Among them closed/bankrupt:\n",
    "Century 21 Stores, Flywheel Sports, Gordmans, Pier 1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f878510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FB_interest_id_raw = pd.read_excel(\"FB_interest_id_raw.xlsx\")\n",
    "# get the ones that need maunal input\n",
    "set(brands) - set(FB_interest_id_raw['brand']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d343281",
   "metadata": {},
   "source": [
    "## create json files for data collection\n",
    "Age, gender, location can be easily write manually, only interest id list are created with code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1ff67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FB_interest_id = pd.read_excel(\"FB_interest_id.xlsx\")\n",
    "\n",
    "with open (\"interest_list.json\", \"a\") as file:\n",
    "    for index, row in FB_interest_id.iterrows():\n",
    "        json.dump({\"or\": [row['id']], \"name\": row['brand']}, file)\n",
    "        file.write(',\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d25325",
   "metadata": {},
   "source": [
    "We are not sharing the interest ids of the brands (the content of interest_list.json), as it is not clear from Facebook whether we have the rights to share them. But you should be able to get them with the code provided so far. Then, use the json files as intructed by pysocialwatcher https://github.com/joaopalotti/pySocialWatcher to collect the data. The following cell provides the example json contents to be saved in the correponding json files."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1e508daa",
   "metadata": {},
   "source": [
    "# collecting_urban.json\n",
    "{\"name\": \"all US vs only urban\",\n",
    "  \"geo_locations\": [\n",
    "      { \"name\": \"countries\", \"values\": [\"US\"] },\n",
    "      { \"name\": \"custom_locations\", \"values\": [{\"custom_type\": \"multi_city\",\"country\": \"US\"}]}\n",
    "  ],\n",
    "  \"genders\": [0],\n",
    "  \"ages_ranges\": [\n",
    "      {\"min\":18}\n",
    "  ],\n",
    "  \"interests\": [# put the content of interest_list.json here\n",
    "]\n",
    "}\n",
    "\n",
    "# collecting_gender.json\n",
    "{\"name\": \"gender\",\n",
    "  \"geo_locations\": [\n",
    "      { \"name\": \"countries\", \"values\": [\"US\"] }\n",
    "  ],\n",
    "  \"genders\": [1, 2],\n",
    "  \"ages_ranges\": [\n",
    "      {\"min\":18}\n",
    "  ],\n",
    "\"interests\": [# put the content of interest_list.json here\n",
    "]\n",
    "}\n",
    "\n",
    "# collecting_age.json\n",
    "{\"name\": \"age\",\n",
    "  \"geo_locations\": [\n",
    "      { \"name\": \"countries\", \"values\": [\"US\"] }\n",
    "  ],\n",
    "  \"genders\": [0],\n",
    "  \"ages_ranges\": [\n",
    "      {\"min\":18, \"max\": 24},\n",
    "      {\"min\":25, \"max\": 34},\n",
    "      {\"min\":35, \"max\": 44},\n",
    "      {\"min\":45, \"max\": 54},\n",
    "      {\"min\":55, \"max\": 64},\n",
    "      {\"min\":65}\n",
    "  ],\n",
    " \"interests\": [# put the content of interest_list.json here\n",
    "]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a7d5e6",
   "metadata": {},
   "source": [
    "## collect data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4920dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_collect_list = [\"collecting_urban.json\", \"collecting_gender.json\", \"collecting_age.json\"]\n",
    "collected_files = [\"collected_urban.csv\", \"collected_gender.csv\", \"collected_age.csv\"]\n",
    "\n",
    "for i in range(len(to_collect_list)):\n",
    "    \n",
    "    collecting = to_collect_list[i]\n",
    "    collected = collected_files[i]\n",
    "    \n",
    "    print(\"start\", collecting, \"---\")\n",
    "    watcher = watcherAPI(api_version=\"15.0\", outputname = collected)\n",
    "    watcher.load_credentials_file(\"FB_credentials.csv\")\n",
    "    \n",
    "    df = watcher.run_data_collection(collecting, remove_tmp_files = True)\n",
    "    \n",
    "    print(collected, \"end---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918da7c6",
   "metadata": {},
   "source": [
    "# Data preprocess\n",
    "\n",
    "process the data collected and in the end get two files representing the upper and lower monthly active users. The files contains brands, and the upper/lower bound monthly active users in the different cateories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3630955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pivot(csvfile, key_column):\n",
    "    \"\"\"\n",
    "    input the file name of the csv file and the column name of the variable(age, gender, etc.)\n",
    "    return the cleaned and pivoted version of dataframe for upper and lower mau\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(csvfile, index_col = 0)\n",
    "    df = df[['interests', key_column, 'mau_audience_upper_bound', 'mau_audience_lower_bound']]\n",
    "    df = df.assign(id = df['interests'].str.extract(\"\\[([0-9]+)\\]\")) \n",
    "    \n",
    "    upper = df[['id', 'mau_audience_upper_bound', key_column]]\n",
    "    upper = upper.pivot(index = 'id', columns = key_column, values = 'mau_audience_upper_bound')\n",
    "    upper.columns.name = None\n",
    "    \n",
    "    lower = df[['id', 'mau_audience_lower_bound', key_column]]\n",
    "    lower = lower.pivot(index = 'id', columns = key_column, values = 'mau_audience_lower_bound')\n",
    "    lower.columns.name = None\n",
    "    \n",
    "    return upper, lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "021e0302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process different categories individually\n",
    "upper_geo, lower_geo = clean_pivot(\"collected_urban.csv\", 'geo_locations')\n",
    "upper_geo.columns = ['all_US', 'urban']\n",
    "lower_geo.columns = ['all_US', 'urban']\n",
    "\n",
    "upper_gender, lower_gender = clean_pivot(\"collected_gender.csv\", 'genders')\n",
    "upper_gender.columns = ['male', 'female']\n",
    "lower_gender.columns = ['male', 'female']\n",
    "\n",
    "upper_age, lower_age = clean_pivot(\"collected_age.csv\", 'ages_ranges')\n",
    "upper_age.columns = ['age18_24', 'age25_34', 'age35_44', 'age45_54', 'age55_64', 'age65plus']\n",
    "lower_age.columns = ['age18_24', 'age25_34', 'age35_44', 'age45_54', 'age55_64', 'age65plus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f9ccbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put together\n",
    "upper_mau = pd.concat([upper_geo, upper_age, upper_gender], axis = 1)\n",
    "lower_mau = pd.concat([lower_geo, lower_age, lower_gender], axis = 1)\n",
    "\n",
    "FB_interet_id = pd.read_excel(\"FB_interest_id.xlsx\", dtype = {'id': str})[['brand', 'id']]\n",
    "upper_mau = pd.merge(FB_interet_id, upper_mau, on = 'id').drop(columns = ['id'])\n",
    "lower_mau = pd.merge(FB_interet_id, lower_mau, on = 'id').drop(columns = ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b62a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find not valid case where the upper and lower bound for all US are both 1000\n",
    "not_valid = pd.merge(upper_mau[['brand', 'all_US']], lower_mau[['brand', 'all_US']], on = 'brand')\n",
    "not_valid = not_valid.set_index('brand')\n",
    "not_valid = not_valid == 1000\n",
    "not_valid = list(not_valid[not_valid.sum(axis = 1) == 2].index.values)\n",
    "\n",
    "# delete\n",
    "upper_mau = upper_mau[~upper_mau['brand'].isin(not_valid)]\n",
    "lower_mau = lower_mau[~lower_mau['brand'].isin(not_valid)]\n",
    "\n",
    "# save\n",
    "upper_mau.to_csv('upper_mau.csv', index = False)\n",
    "lower_mau.to_csv('lower_mau.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
